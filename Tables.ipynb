{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8eb8996",
   "metadata": {},
   "source": [
    "# Table 1: Test Retest stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37cc5b",
   "metadata": {},
   "source": [
    "## Manuscript Information\n",
    " \n",
    "\"Active Mutual Conjoint Estimation of Multiple Contrast Sensitivity\n",
    "Functions\"\n",
    "Dom CP Marticorena, Quinn Wai Wong, Jake Browning, Ken Wilbur, Pinakin Davey, Aaron R. Seitz, Jacob R. Gardner, Dennis L. Barbour\n",
    "_Journal of Vision_\n",
    "\n",
    "[link to paper or preprint]\n",
    "\n",
    "## Lab and Institution Information\n",
    "\n",
    "NeuroMedical Informatics Lab  \n",
    "Washington University in St. Louis\n",
    "\n",
    "## Figure Description\n",
    "\n",
    "Test retest stuff\n",
    "\n",
    "## References\n",
    "\n",
    "[references]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90310c1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b6c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gpytorch as gp\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "# need access to root directory to import utils\n",
    "parent_dir = os.path.dirname(os.path.abspath(''))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    \n",
    "from utility.utils import *\n",
    "from QuickCSF import QuickCSF, simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262e8c2",
   "metadata": {},
   "source": [
    "### Check versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3813b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version -->> 3.10.9 (expected 3.10.9)\n",
      "gpytorch version -->> 1.8.1 (expected 1.8.1)\n",
      "pytorch version -->> 1.13.1 (expected 1.13.1)\n"
     ]
    }
   ],
   "source": [
    "print(\"python version -->>\", sys.version.split(\" \")[0], \"(expected 3.10.9)\")\n",
    "print(\"gpytorch version -->>\", gp.__version__, \"(expected 1.8.1)\")\n",
    "print(\"pytorch version -->>\", torch.__version__, \"(expected 1.13.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ec4f9",
   "metadata": {},
   "source": [
    "### Run-time flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481a8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qcsf_train_mode = False\n",
    "train_mode = False          # create new data? Set to false if plotting existing results\n",
    "verbose_mode = True         # print verbose analyses?\n",
    "scrn_mode = True            # plot on screen?\n",
    "save_results_mode = True    # save results to file?\n",
    "save_plots_mode = True      # save plots to directory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de700e8",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5e73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data selection\n",
    "jigo_file = '../data/raw/representative_curves.json'\n",
    "representative_curves = load_json_from_file(jigo_file)\n",
    "all_phenotypes = {\n",
    "    'Quantile 1': representative_curves['quantile1-pid2-cue1-ecc3'],\n",
    "    'Quantile 2': representative_curves['quantile2-pid1-cue1-ecc0'],\n",
    "    'Quantile 3': representative_curves['quantile3-pid2-cue0-ecc2'],\n",
    "    'Quantile 4': representative_curves['quantile4-pid3-cue0-ecc1'],\n",
    "    'Quantile 5': representative_curves['quantile5-pid7-cue1-ecc3'],\n",
    "    'Quantile 6': representative_curves['quantile6-pid0-cue0-ecc0'],\n",
    "    'Quantile 7': representative_curves['quantile7-pid0-cue1-ecc0'],\n",
    "    'Quantile 8': representative_curves['quantile8-pid1-cue1-ecc3'],\n",
    "    'Quantile 9': representative_curves['quantile9-pid5-cue1-ecc0'],\n",
    "    'Quantile 10': representative_curves['quantile10-pid8-cue0-ecc3'],\n",
    "    'Quantile 11': representative_curves['quantile11-pid5-cue0-ecc1'],\n",
    "    'Quantile 12': representative_curves['quantile12-pid8-cue1-ecc2'],\n",
    "    'Quantile 13': representative_curves['quantile13-pid4-cue0-ecc1'],\n",
    "    'Quantile 14': representative_curves['quantile14-pid7-cue0-ecc1'],\n",
    "    'Quantile 15': representative_curves['quantile15-pid3-cue1-ecc2'],\n",
    "    'Quantile 16': representative_curves['quantile16-pid0-cue0-ecc1'],\n",
    "    'Quantile 17': representative_curves['quantile17-pid8-cue1-ecc0'],\n",
    "    'Quantile 18': representative_curves['quantile18-pid8-cue0-ecc1'],\n",
    "    'Quantile 19': representative_curves['quantile19-pid0-cue1-ecc1'],\n",
    "    'Quantile 20': representative_curves['quantile20-pid4-cue0-ecc2']\n",
    "}\n",
    "\n",
    "def create_quantile_variables(num_quantiles):\n",
    "    \"\"\"\n",
    "    Create a list of quantile names based on the number of quantiles.\n",
    "    :param num_quantiles: Total number of quantiles.\n",
    "    :return: List of quantile names.\n",
    "    \"\"\"\n",
    "    return [f'Quantile {i+1}' for i in range(num_quantiles)]\n",
    "\n",
    "# tasks and number of latents\n",
    "num_latents = 2\n",
    "\n",
    "# fixed unless something nutso happens \n",
    "num_tasks = num_latents\n",
    "\n",
    "sampling_method = 'alternating'     # 'alternating' or 'unconstrained'\n",
    "weight_decay = 1e-4\n",
    "num_quantiles = 20\n",
    "\n",
    "# Create \"enums\"\n",
    "create_quantile_variables(num_quantiles)\n",
    "\n",
    "# Configure for all unique pairs\n",
    "num_pairs = comb(num_quantiles, 2) + num_quantiles\n",
    "\n",
    "# choose to run multiple experiments with preset random seeds\n",
    "# or a single experiment specifying your own random seeds\n",
    "run_multiple_experiments = True\n",
    "\n",
    "# list of zeros as long as num_latents\n",
    "primer_random_seeds = [0, 0]\n",
    "gp_random_seed = 0\n",
    "\n",
    "# number of samples\n",
    "num_halton_samples_per_task = 2\n",
    "num_new_pts_per_task = 98\n",
    "\n",
    "# choosing which figures to make\n",
    "make_gp_gifs = True\n",
    "make_entropy_gifs = False\n",
    "make_hyper_plots = True\n",
    "\n",
    "# directory to save plots and results\n",
    "# will save to the path <save_dir_prefix>/<current_timestamp>\n",
    "save_dir_prefix = 'analysis/Tables'\n",
    "\n",
    "# Individual print flags \n",
    "print_training_hyperparameters = False\n",
    "print_training_iters = False\n",
    "print_progress_bar = False\n",
    "\n",
    "# Set all to true if verbose_mode\n",
    "if verbose_mode:\n",
    "    print_training_hyperparameters = False\n",
    "    print_training_iters = True\n",
    "    print_progress_bar = False\n",
    "    \n",
    "# Create the bounds for the data\n",
    "raw_freq_min = .5\n",
    "raw_freq_max = 32\n",
    "raw_contrast_min = 1e-3\n",
    "raw_contrast_max = 1\n",
    "\n",
    "x_tick_labels = [.5, 2, 8, 32]\n",
    "y_tick_labels = [1, 0.1, 0.01, 0.001]\n",
    "\n",
    "# Define how to transform the data\n",
    "x_min = logFreq().forward(raw_freq_min)\n",
    "x_max = logFreq().forward(raw_freq_max)\n",
    "y_min = logContrast().forward(raw_contrast_max)  # max and min get flipped when inverting\n",
    "y_max = logContrast().forward(raw_contrast_min)\n",
    "\n",
    "# transform the data\n",
    "def normalize_to_unit_range(d):\n",
    "    return scale_data_within_range(d, (0, 1), x_min, x_max, y_min, y_max)\n",
    "\n",
    "# marginal log resolutions of evaulation grid\n",
    "x_resolution = 15  # 15 spatial frequencies per octave\n",
    "y_resolution = 30  # 30 contrast units per decade\n",
    "\n",
    "# for computing the proper prior threshold curve\n",
    "psi_gamma  = 0.04  # guess rate is 4%\n",
    "psi_lambda = 0.04 # lapse rate is 4%\n",
    "psi_sigma = 0.08\n",
    "sigmoid_type = 'logistic'\n",
    "\n",
    "# training parameters?\n",
    "num_initial_points_training_iters = 500\n",
    "num_new_points_training_iters = 150\n",
    "train_on_all_points_iters = 1500\n",
    "sampling_strategy = 'active'\n",
    "mean_module = 'constant_mean'\n",
    "train_on_all_points_after_sampling = False\n",
    "calculate_rmse = True\n",
    "calculate_posterior = True\n",
    "calculate_entropy = True\n",
    "\n",
    "# GP hyperparameters?\n",
    "learning_rate = .125\n",
    "beta_for_regularization = .5\n",
    "min_lengthscale = .15 # Note this changed from .2\n",
    "\n",
    "# Set raw ghost points\n",
    "raw_ghost_frequency = np.array([1, 2, 4, 8, 16, 32, 64, 128])\n",
    "raw_ghost_contrast = np.array([5e-4, 5e-4, 5e-4, 5e-4, 5e-4, 5e-4, 5e-4, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2f1ae",
   "metadata": {},
   "source": [
    "### Set Timestamp to Save To"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd17b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to... analysis/Tables/2024-05-15_10-06-49/\n"
     ]
    }
   ],
   "source": [
    "# get current timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_dir = f\"{save_dir_prefix}/{timestamp}/\"\n",
    "ensure_directory_exists(save_dir)\n",
    "print(f\"Saving to... {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b391aee",
   "metadata": {},
   "source": [
    "### Automatically calculated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd88a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create common variables shared among all tasks\n",
    "grid, xx, yy, xs, ys = create_evaluation_grid_resolution(\n",
    "        x_min, x_max, y_min, y_max, x_resolution, y_resolution)\n",
    "\n",
    "grid_transformed = transform_dataset(grid, phi=normalize_to_unit_range)\n",
    "def create_cubic_spline(curve):\n",
    "    \"\"\"\n",
    "  This method creates a cubic spline to approximate the given curve.\n",
    "  :param curve: A nx2 numpy matrix. First column is x values. Second column is y values.\n",
    "  :return: The cubic spline.\n",
    "  \"\"\"\n",
    "    x = curve[:, 0]\n",
    "    y = curve[:, 1]\n",
    "\n",
    "    cs = CubicSpline(x, y, extrapolate=False)\n",
    "    return cs\n",
    "\n",
    "# function to get ground truth curves\n",
    "def get_spline(curve):\n",
    "    \"\"\"\n",
    "    Modifies the curve by fitting a parabola through the maximum and the last points, then creates a cubic spline.\n",
    "    The parabola's x-intercept is added to the curve if it is to the right of the last point.\n",
    "    If the x-intercept is not to the right, the curve is not modified.\n",
    "    :param curve: A list of [x, y] points.\n",
    "    :return: The cubic spline.\n",
    "    \"\"\"\n",
    "    curve = np.array(curve)\n",
    "    curve[:, 0] = (np.log2(10) * curve[:, 0]) - np.log2(0.125)\n",
    "\n",
    "    # Find the max y value and its corresponding x value\n",
    "    max_y = np.max(curve[:, 1])\n",
    "    max_y_index = np.argmax(curve[:, 1])\n",
    "    x_max_y = curve[max_y_index, 0]\n",
    "\n",
    "    # Use the last real point of the curve\n",
    "    x_last, y_last = curve[-1, 0], curve[-1, 1]\n",
    "\n",
    "    # Fit the parabola through the max and last point\n",
    "    a = (y_last - max_y) / ((x_last - x_max_y)**2)\n",
    "    coeffs = [a, -2*a*x_max_y, a*x_max_y**2 + max_y]\n",
    "    roots = np.roots(coeffs)\n",
    "    x_intercept = roots[0]  # Choose the first root\n",
    "\n",
    "    # Add the intercept point to the curve only if it's to the right of the last point\n",
    "    if x_intercept > x_last:\n",
    "        curve = np.vstack([curve, [x_intercept, 0]])\n",
    "        curve = curve[curve[:, 0].argsort()]  # Sorting by x values\n",
    "\n",
    "    # Create and return the cubic spline\n",
    "    cs = create_cubic_spline(curve)\n",
    "    return cs\n",
    "\n",
    "# get number of actively learned points for conjoint\n",
    "num_pts_per_task = num_new_pts_per_task + num_halton_samples_per_task\n",
    "num_new_conjoint_pts = num_new_pts_per_task * num_tasks\n",
    "num_conjoint_pts = num_pts_per_task * num_tasks\n",
    "\n",
    "# create ghost points and labels\n",
    "ghost_x1 = logFreq().forward(raw_ghost_frequency)\n",
    "ghost_x2 = logContrast().forward(raw_ghost_contrast)\n",
    "assert len(ghost_x1) == len(ghost_x2), \"x1 and x2 have diff lengths\"\n",
    "\n",
    "ghost_X = np.vstack((ghost_x1, ghost_x2)).T\n",
    "ghost_y = np.array([0]*len(ghost_x2))\n",
    "\n",
    "# create disjoint initial primer points\n",
    "halton_X = get_halton_samples(xx, yy, num_halton_samples_per_task)\n",
    "initial_disjoint_X = np.vstack((ghost_X, halton_X))\n",
    "\n",
    "# create conjoint initial primer points and task indices\n",
    "# simulated Halton y labels are created later for each experiment\n",
    "halton_Xs = np.repeat(halton_X, num_tasks, axis=0)\n",
    "\n",
    "num_ghost_points_per_task = len(ghost_y)\n",
    "halton_task_indices = torch.arange(num_tasks).tile((num_halton_samples_per_task))\n",
    "\n",
    "ghost_Xs = np.tile(ghost_X, (num_tasks, 1))\n",
    "ghost_ys = np.tile(ghost_y, num_tasks)\n",
    "ghost_task_indices = torch.arange(num_tasks).repeat_interleave(num_ghost_points_per_task)\n",
    "\n",
    "num_disjoint_ghost_points = len(ghost_x2)\n",
    "num_conjoint_ghost_points = len(ghost_task_indices)\n",
    "\n",
    "# create initial dataset using ghost and halton samples\n",
    "initial_Xs = np.vstack((ghost_Xs, halton_Xs))\n",
    "initial_task_indices = torch.cat((ghost_task_indices, halton_task_indices))\n",
    "\n",
    "# unique random seeds for halton samples and training\n",
    "primer_seeds_list = [(np.arange(num_tasks) + num_tasks*i).tolist() for i in range(num_pairs)]\n",
    "gp_seed_list = [(np.arange(num_tasks) + num_tasks*i).tolist() for i in range(num_pairs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb8db4f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865ca87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_dict = {\n",
    "    \"num_latents\": num_latents,\n",
    "    \"sampling_method\": sampling_method,\n",
    "    \"num_pairs\": num_pairs,\n",
    "    \"num_halton_samples_per_task\": num_halton_samples_per_task,\n",
    "    \"num_new_pts_per_task\": num_new_pts_per_task,\n",
    "    \"raw_freq_min\": raw_freq_min,\n",
    "    \"raw_freq_max\": raw_freq_max,\n",
    "    \"raw_contrast_min\": raw_contrast_min,\n",
    "    \"raw_contrast_max\": raw_contrast_max,\n",
    "    \"x_resolution\": x_resolution,\n",
    "    \"y_resolution\": y_resolution,\n",
    "    \"raw_ghost_frequency\": raw_ghost_frequency.tolist(),\n",
    "    \"raw_ghost_contrast\": raw_ghost_contrast.tolist(),\n",
    "    \"primer_seeds_list\": primer_seeds_list,\n",
    "    \"gp_seed_list\": gp_seed_list,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"min_lengthscale\": min_lengthscale,\n",
    "    \"psi_sigma\": psi_sigma,\n",
    "    \"sigmoid_type\": sigmoid_type,\n",
    "    \"psi_gamma\": psi_gamma,\n",
    "    \"psi_lambda\": psi_lambda,\n",
    "    \"lr\": learning_rate,\n",
    "    \"num_initial_training_iters\": num_initial_points_training_iters,\n",
    "    \"num_new_points_training_iters\": num_new_points_training_iters,\n",
    "    \"beta_for_regularization\": beta_for_regularization,\n",
    "    \"train_on_all_points_after_sampling\": train_on_all_points_after_sampling,\n",
    "    \"print_training_hyperparameters\": print_training_hyperparameters,\n",
    "    \"print_training_iters\": print_training_iters,\n",
    "    \"progress_bar\": print_progress_bar,\n",
    "    \"calculate_rmse\": calculate_rmse,\n",
    "    \"calculate_entropy\": calculate_entropy,\n",
    "    \"calculate_posterior\": calculate_posterior\n",
    "}\n",
    "\n",
    "if train_mode:\n",
    "    ensure_directory_exists(save_dir)\n",
    "    with open(save_dir + \"run_configs.json\", 'w') as file:\n",
    "        json.dump(figure_dict, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edad6ed",
   "metadata": {},
   "source": [
    "### Disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab28c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    for run in range(2):\n",
    "        print(f\"Run: {run}\")\n",
    "        disjoint_results_dicts = [{} for _ in range(num_tasks)]\n",
    "        disjoint_gp_lists = []\n",
    "\n",
    "        all_phenotypes_keys = list(all_phenotypes.keys()) # TODO  # Getting all the keys from your dictionary\n",
    "\n",
    "        # Using itertools to get all unique combinations of length 2 (pairs)\n",
    "        phenotype_pairs = combinations_with_replacement(all_phenotypes_keys, 2)\n",
    "\n",
    "        # initialize pair index to iterate using actual numbers\n",
    "        pair_index = 0\n",
    "\n",
    "        # Now iterate over the first 'num_pairs' pairs\n",
    "        for pair in phenotype_pairs:\n",
    "\n",
    "            phenotype_pair = [(pheno, all_phenotypes[pheno]) for pheno in pair]\n",
    "\n",
    "            print(f\"Pair {pair}\")\n",
    "\n",
    "            # get unique random seeds for each exp\n",
    "            run_seed_modifier = 8888*run\n",
    "            primer_seeds = [s + run_seed_modifier for s in primer_seeds_list[pair_index]]\n",
    "            gp_seeds = [s + run_seed_modifier for s in gp_seed_list[pair_index]]\n",
    "\n",
    "            for i, (pheno, _) in enumerate(phenotype_pair):\n",
    "\n",
    "                print(pheno)\n",
    "\n",
    "                ground_truths = [get_spline(pheno_data) for _, pheno_data in phenotype_pair]\n",
    "\n",
    "                cs = ground_truths[i]\n",
    "                primer_seed = primer_seeds[i]\n",
    "                gp_seed = gp_seeds[i]\n",
    "\n",
    "                # get initial primer labels\n",
    "                set_random_seed(primer_seed)\n",
    "                halton_y = simulate_labeling(halton_X[:,0], halton_X[:,1], cs, 0, 0, sigmoid_type=sigmoid_type, psi_sigma=psi_sigma) # changed  psi_gamma, psi_lambda to 0, 0\n",
    "                initial_y = np.hstack((ghost_y, halton_y))\n",
    "\n",
    "                # run active learning\n",
    "                set_random_seed(gp_seed)\n",
    "\n",
    "                model, likelihood, X, y, rmse_list, _, posterior_list, _ = sample_and_train_gp(\n",
    "                    cs,\n",
    "                    grid,\n",
    "                    xx,\n",
    "                    yy,\n",
    "                    sampling_strategy=sampling_strategy,\n",
    "                    mean_module_name=mean_module,\n",
    "                    psi_sigma=psi_sigma,\n",
    "                    sigmoid_type=sigmoid_type,\n",
    "                    psi_gamma=psi_gamma,\n",
    "                    psi_lambda=psi_lambda,\n",
    "                    lr=learning_rate,\n",
    "                    num_initial_training_iters=num_initial_points_training_iters,\n",
    "                    num_new_points_training_iters=num_new_points_training_iters,\n",
    "                    num_new_points=num_new_pts_per_task,\n",
    "                    beta_for_regularization=beta_for_regularization,\n",
    "                    train_on_all_points_after_sampling=train_on_all_points_after_sampling,\n",
    "                    train_on_all_points_iters=train_on_all_points_iters,\n",
    "                    phi=normalize_to_unit_range,\n",
    "                    print_training_hyperparameters=print_training_hyperparameters,\n",
    "                    print_training_iters=print_training_iters,\n",
    "                    progress_bar=print_progress_bar,\n",
    "                    min_lengthscale=min_lengthscale,\n",
    "                    calculate_rmse=calculate_rmse,\n",
    "                    calculate_entropy=calculate_entropy,\n",
    "                    calculate_posterior=calculate_posterior,\n",
    "                    initial_Xs=initial_disjoint_X,\n",
    "                    initial_ys=initial_y,\n",
    "                    num_ghost_points=num_disjoint_ghost_points,\n",
    "                    weight_decay=weight_decay\n",
    "                )\n",
    "\n",
    "\n",
    "                zz = evaluate_posterior_mean(model, likelihood, grid_transformed) \\\n",
    "                        .reshape(xx.shape)\n",
    "                \n",
    "                level = (1 - psi_lambda + psi_gamma) / 2\n",
    "                GP_level_curve_y_values_on_grid_cols = []\n",
    "                for intermediate_model, intermediate_likelihood in posterior_list:\n",
    "                    intermediate_zz = evaluate_posterior_mean(\n",
    "                        intermediate_model, \n",
    "                        intermediate_likelihood, \n",
    "                        grid_transformed\n",
    "                    ).reshape(xx.shape)\n",
    "                    \n",
    "                    zzmin = (intermediate_zz[:, :] - level) ** 2\n",
    "                    level_curve_indices = np.int64(np.argmin(zzmin, axis=0))\n",
    "                    level_curve = yy[:, 0][level_curve_indices[:]]\n",
    "                    GP_level_curve_y_values_on_grid_cols.append(level_curve)\n",
    "                    \n",
    "                ground_truth_y_values_on_grid_cols = cs(xx[0, :])\n",
    "\n",
    "                disjoint_results_dicts[i][pair_index] = {\n",
    "                    'training_seed': gp_seed,\n",
    "                    'random_seed': primer_seed,\n",
    "                    'X': X,\n",
    "                    'y': y,\n",
    "                    'zz': zz,\n",
    "                    'rmse_list': rmse_list,\n",
    "                    'GP_level_curve_y_values_on_grid_cols': GP_level_curve_y_values_on_grid_cols,\n",
    "                    'ground_truth_y_values_on_grid_cols': ground_truth_y_values_on_grid_cols\n",
    "                }\n",
    "\n",
    "                gif_dict = {\n",
    "                    'xx': xx,\n",
    "                    'yy': yy,\n",
    "                    'X': X,\n",
    "                    'y': y,\n",
    "                    'cs': cs,\n",
    "                    'psi_sigma': psi_sigma,\n",
    "                    'sigmoid_type': sigmoid_type,\n",
    "                    'psi_gamma': psi_gamma,\n",
    "                    'psi_lambda': psi_lambda,\n",
    "                    'x_min': x_min,\n",
    "                    'x_max': x_max,\n",
    "                    'y_min': y_min,\n",
    "                    'y_max': y_max,\n",
    "                    'xs': xs,\n",
    "                    'ys': ys,\n",
    "                    'grid': grid,\n",
    "                    'f': normalize_to_unit_range,\n",
    "                    'posterior_list': posterior_list,\n",
    "                }\n",
    "\n",
    "                # create gif of the task\n",
    "                if make_gp_gifs and save_plots_mode:\n",
    "                    ntitle = pheno.replace(' ', '_')\n",
    "                    gif_path = f'{save_dir}run_{run}/task{i}_{ntitle.lower()}/{pair_index}/'\n",
    "                    ensure_directory_exists(gif_path)\n",
    "                    create_and_save_plots(gif_dict, gif_path, ntitle, start_index=num_disjoint_ghost_points,\n",
    "                                xticks_labels=x_tick_labels, yticks_labels=y_tick_labels)\n",
    "                    create_gif(gif_path)\n",
    "\n",
    "                gp_list = [model for model, _ in posterior_list]\n",
    "                disjoint_gp_lists.append(gp_list)\n",
    "                print()\n",
    "\n",
    "            # Increment the index for the next iteration\n",
    "            pair_index += 1\n",
    "            print(pair_index)\n",
    "\n",
    "            if save_results_mode:\n",
    "                full_save_dir = f'{save_dir}run_{run}'\n",
    "                ensure_directory_exists(full_save_dir)\n",
    "                with open(f'{full_save_dir}/disjoint_results.pkl', 'wb') as file:\n",
    "                    pickle.dump(disjoint_results_dicts, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528fdde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = disjoint_results_dicts\n",
    "# # a[task index][pair index][dict key]\n",
    "# print(a[0][0].keys())\n",
    "# # ['GP_level_curve_y_values_on_grid_cols'][num points collected][x value]\n",
    "# print(len(a[0][0]['GP_level_curve_y_values_on_grid_cols']))\n",
    "# print(a[0][0]['GP_level_curve_y_values_on_grid_cols'][7])\n",
    "# print(a[0][0]['ground_truth_y_values_on_grid_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb5462b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(11):\n",
    "    \n",
    "#     yvals = a[0][0]['GP_level_curve_y_values_on_grid_cols'][i]\n",
    "#     GT = a[0][0]['ground_truth_y_values_on_grid_cols']\n",
    "    \n",
    "#     # Generate x values based on the index of yvals\n",
    "#     x = np.arange(len(yvals))\n",
    "\n",
    "#     # Plot the curve\n",
    "#     plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "#     plt.plot(x, yvals, label='yvals')  # Plot x vs. yvals with a label for the legend\n",
    "#     plt.plot(x, GT, label='GT')\n",
    "#     plt.xlabel('Index')  # Set the x-axis label\n",
    "#     plt.ylabel('y-values')  # Set the y-axis label\n",
    "#     plt.title('Plot of y-values vs. Index')  # Set the title of the plot\n",
    "#     plt.legend()  # Show the legend\n",
    "#     plt.grid(True)  # Show a grid\n",
    "\n",
    "#     # Customize the tick labels for x and y axes\n",
    "#     plt.yticks([0, 1, 2, 3])\n",
    "\n",
    "#     plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515b09f",
   "metadata": {},
   "source": [
    "### Conjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9788269",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    for run in range(2):\n",
    "        print(f\"Run: {run}\")\n",
    "        conjoint_results_dicts = {}\n",
    "        conjoint_gp_lists = []\n",
    "\n",
    "        all_phenotypes_keys = list(all_phenotypes.keys()) # TODO # Getting all the keys from your dictionary\n",
    "\n",
    "        # Using itertools to get all unique combinations of length 2 (pairs)\n",
    "        phenotype_pairs = combinations_with_replacement(all_phenotypes_keys, 2)\n",
    "\n",
    "        # initialize pair index to iterate using actual numbers\n",
    "        pair_index = 0\n",
    "\n",
    "        # Now iterate over the first 'num_pairs' pairs\n",
    "        for pair in phenotype_pairs:\n",
    "\n",
    "            phenotype_pair = [(pheno, all_phenotypes[pheno]) for pheno in pair]\n",
    "\n",
    "            print(f\"Pair {pair}\")\n",
    "\n",
    "            # get unique random seeds for each exp\n",
    "            run_seed_modifier = 8888*run\n",
    "            primer_seeds = [s + run_seed_modifier for s in primer_seeds_list[pair_index]]\n",
    "            gp_seeds = [s + run_seed_modifier for s in gp_seed_list[pair_index]]\n",
    "            gp_seed = gp_seeds[0]\n",
    "\n",
    "            # get unique halton labels for each experiment\n",
    "            halton_y_list = []\n",
    "\n",
    "            for i, _ in enumerate(phenotype_pair):\n",
    "                ground_truths = [get_spline(pheno_data) for _, pheno_data in phenotype_pair]\n",
    "                cs = ground_truths[i]\n",
    "                set_random_seed(primer_seeds[i])\n",
    "                halton_y = simulate_labeling(halton_X[:,0], halton_X[:,1], cs, 0, 0, sigmoid_type=sigmoid_type, psi_sigma=psi_sigma)  # changed  psi_gamma, psi_lambda to 0, 0\n",
    "                halton_y_list.append(halton_y)\n",
    "\n",
    "            # stack all task halton labels then stack ghost with halton labels\n",
    "            halton_ys = np.array([y for y_per_halton in zip(*halton_y_list) for y in y_per_halton])\n",
    "            initial_ys = np.hstack((ghost_ys, halton_ys))\n",
    "\n",
    "            # run active learning\n",
    "            set_random_seed(gp_seed)\n",
    "\n",
    "            model, likelihood, X, y, task_indices, rmse_list, entropy_list, posterior_list, _ = sample_and_train_gp_conjoint(\n",
    "                cs=ground_truths,\n",
    "                grid=grid,\n",
    "                xx=xx,\n",
    "                yy=yy,\n",
    "                psi_sigma=psi_sigma,\n",
    "                psi_gamma=psi_gamma,\n",
    "                psi_lambda=psi_lambda,\n",
    "                lr=learning_rate,\n",
    "                num_initial_training_iters=num_initial_points_training_iters,\n",
    "                num_new_points_training_iters=num_new_points_training_iters,\n",
    "                num_new_points=num_new_conjoint_pts,\n",
    "                beta_for_regularization=beta_for_regularization,\n",
    "                phi=normalize_to_unit_range,\n",
    "                print_training_hyperparameters=print_training_hyperparameters,\n",
    "                print_training_iters=print_training_iters,\n",
    "                train_on_all_points_after_sampling=train_on_all_points_after_sampling,\n",
    "                train_on_all_points_iters=train_on_all_points_iters,\n",
    "                min_lengthscale=min_lengthscale,\n",
    "                initial_Xs=initial_Xs,\n",
    "                initial_ys=initial_ys,\n",
    "                sampling_strategy=sampling_strategy,\n",
    "                num_ghost_points=num_conjoint_ghost_points,\n",
    "                calculate_rmse=calculate_rmse,\n",
    "                calculate_entropy=calculate_entropy,\n",
    "                calculate_posterior=calculate_posterior,\n",
    "                progress_bar=print_progress_bar,\n",
    "                num_tasks=num_tasks,\n",
    "                num_latents=num_latents,\n",
    "                task_indices=initial_task_indices,\n",
    "                sampling_method=sampling_method,\n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "\n",
    "            zz = evaluate_posterior_mean(model, likelihood, grid_transformed) \\\n",
    "                .reshape((*xx.shape, num_tasks))\n",
    "            \n",
    "            level = (1 - psi_lambda + psi_gamma) / 2\n",
    "            GP_level_curve_y_values_on_grid_cols = [list(), list()]\n",
    "            for intermediate_model, intermediate_likelihood in posterior_list:\n",
    "                intermediate_zz = evaluate_posterior_mean(\n",
    "                    intermediate_model, \n",
    "                    intermediate_likelihood, \n",
    "                    grid_transformed\n",
    "                ).reshape((*xx.shape, num_tasks))\n",
    "                \n",
    "                for task_index in range(num_tasks):\n",
    "                    zzmin = (intermediate_zz[:,:,task_index] - level) ** 2\n",
    "                    level_curve_indices = np.int64(np.argmin(zzmin, axis=0))\n",
    "                    level_curve = yy[:, 0][level_curve_indices[:]]\n",
    "                    GP_level_curve_y_values_on_grid_cols[task_index].append(level_curve)\n",
    "                                \n",
    "            ground_truth_y_values_on_grid_cols = []\n",
    "            for task_index in range(num_tasks):\n",
    "                ground_truth_y_values_on_grid_cols.append(ground_truths[task_index](xx[0, :]))\n",
    "\n",
    "            conjoint_results_dicts[pair_index] = {\n",
    "                'training_seed': gp_seed,\n",
    "                'random_seeds': primer_seeds,\n",
    "                'X': X,\n",
    "                'y': y,\n",
    "                'zz': zz,\n",
    "                'task_indices': task_indices,\n",
    "                'entropy_list': entropy_list,\n",
    "                'rmse_list': rmse_list,\n",
    "                'GP_level_curve_y_values_on_grid_cols': GP_level_curve_y_values_on_grid_cols,\n",
    "                'ground_truth_y_values_on_grid_cols': ground_truth_y_values_on_grid_cols\n",
    "            }\n",
    "\n",
    "            gp_list = [model for model, _ in posterior_list]\n",
    "            conjoint_gp_lists.append(gp_list)\n",
    "            print()\n",
    "\n",
    "            # Increment the index for the next iteration\n",
    "            pair_index += 1\n",
    "            print(pair_index)\n",
    "\n",
    "            if save_results_mode:\n",
    "                full_save_dir = f'{save_dir}run_{run}'\n",
    "                ensure_directory_exists(save_dir)\n",
    "                with open(f'{full_save_dir}/conjoint_results.pkl', 'wb') as file:\n",
    "                    pickle.dump(conjoint_results_dicts, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc10412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = conjoint_results_dicts\n",
    "# # a[pair index][dict key][task index]\n",
    "# print(a[0].keys())\n",
    "# # ['GP_level_curve_y_values_on_grid_cols'][num points sampled][x value]\n",
    "# print(len(a[0]['GP_level_curve_y_values_on_grid_cols'][0]))\n",
    "# print(a[0]['GP_level_curve_y_values_on_grid_cols'][0][7])\n",
    "# print(a[0]['ground_truth_y_values_on_grid_cols'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a45c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(21):\n",
    "    \n",
    "#     yvals = a[0]['GP_level_curve_y_values_on_grid_cols'][0][i]\n",
    "#     GT = a[0]['ground_truth_y_values_on_grid_cols'][0]\n",
    "#     # Generate x values based on the index of yvals\n",
    "#     x = np.arange(len(yvals))\n",
    "\n",
    "#     # Plot the curve\n",
    "#     plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "#     plt.plot(x, yvals, label='yvals')  # Plot x vs. yvals with a label for the legend\n",
    "#     plt.plot(x, GT, label='GT')\n",
    "#     plt.xlabel('Index')  # Set the x-axis label\n",
    "#     plt.ylabel('y-values')  # Set the y-axis label\n",
    "#     plt.title('Plot of y-values vs. Index')  # Set the title of the plot\n",
    "#     plt.legend()  # Show the legend\n",
    "#     plt.grid(True)  # Show a grid\n",
    "\n",
    "#     # Customize the tick labels for x and y axes\n",
    "#     plt.yticks([0, 1, 2, 3])\n",
    "\n",
    "#     plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b1cc6",
   "metadata": {},
   "source": [
    "### qCSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e2ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if qcsf_train_mode:\n",
    "    for run in range(2):\n",
    "        print(f\"Run: {run}\")\n",
    "        qcsf_results_dicts = [{} for _ in range(num_tasks)]\n",
    "        qcsf_gp_lists = []\n",
    "\n",
    "        all_phenotypes_keys = list(all_phenotypes.keys()) # TODO  # Getting all the keys from your dictionary\n",
    "\n",
    "        # Using itertools to get all unique combinations of length 2 (pairs)\n",
    "        phenotype_pairs = combinations_with_replacement(all_phenotypes_keys, 2)\n",
    "\n",
    "        # initialize pair index to iterate using actual numbers\n",
    "        pair_index = 0\n",
    "\n",
    "        # Now iterate over the first 'num_pairs' pairs\n",
    "        for pair in phenotype_pairs:\n",
    "\n",
    "            phenotype_pair = [(pheno, all_phenotypes[pheno]) for pheno in pair]\n",
    "\n",
    "            print(f\"Pair {pair}\")\n",
    "\n",
    "            # get unique random seeds for each exp\n",
    "            run_seed_modifier = 8888*run\n",
    "            primer_seeds = [s + run_seed_modifier for s in primer_seeds_list[pair_index]]\n",
    "\n",
    "            for i, (pheno, _) in enumerate(phenotype_pair):\n",
    "\n",
    "                print(pheno)\n",
    "\n",
    "                ground_truths = [get_spline(pheno_data) for _, pheno_data in phenotype_pair]\n",
    "\n",
    "                cs = ground_truths[i]\n",
    "                primer_seed = primer_seeds[i]\n",
    "\n",
    "                # get initial primer labels\n",
    "                set_random_seed(primer_seed)\n",
    "\n",
    "                simulationParams = {\n",
    "                    'trials': 100,\n",
    "                    'stimuli': {\n",
    "                        'minContrast': raw_contrast_min,\n",
    "                        'maxContrast': raw_contrast_max,\n",
    "                        'contrastResolution': xs,\n",
    "                        'minFrequency': raw_freq_min,\n",
    "                        'maxFrequency': raw_freq_max,\n",
    "                        'frequencyResolution': ys,\n",
    "                    },\n",
    "                    'parameters': None,\n",
    "                    'd': 1,  # guess rate used by qcsf model           \n",
    "                    'psiGamma': 0,\n",
    "                    'psiLambda': 0,\n",
    "                    'psiSigma': psi_sigma,\n",
    "                    \"sigmoidType\": sigmoid_type,\n",
    "                    'showPlots': False  # prevents plt from plotting every run \n",
    "                }\n",
    "\n",
    "                rmses, times, params, predictions = simulate.runSimulation(\n",
    "                    trueThresholdCurve=np.array(phenotype_pair[i][1]),\n",
    "                    **simulationParams,\n",
    "                    return_intermediate_predictions=True\n",
    "                )        \n",
    "                \n",
    "                qcsf_y_values_on_grid_cols = [[0 for _ in range(predictions[0].shape[0])]] + predictions\n",
    "                    \n",
    "                ground_truth_y_values_on_grid_cols = cs(xx[0, :])\n",
    "\n",
    "                qcsf_results_dicts[i][pair_index] = {\n",
    "                    'random_seed': primer_seed,\n",
    "                    'rmse_list': rmses,\n",
    "                    'GP_level_curve_y_values_on_grid_cols': qcsf_y_values_on_grid_cols,\n",
    "                    'ground_truth_y_values_on_grid_cols': ground_truth_y_values_on_grid_cols\n",
    "                }\n",
    "\n",
    "            # Increment the index for the next iteration\n",
    "            pair_index += 1\n",
    "            print(pair_index)\n",
    "\n",
    "            if save_results_mode:\n",
    "                full_save_dir = f'{save_dir}run_{run}'\n",
    "                ensure_directory_exists(full_save_dir)\n",
    "                with open(f'{full_save_dir}/qcsf_results.pkl', 'wb') as file:\n",
    "                    pickle.dump(qcsf_results_dicts, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99c568ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if qcsf_train_mode:\n",
    "    \n",
    "#     qcsf_results = dict() # index like this: qcsf_results['Quantile 1']['rmses'], this will return a list of rmse values\n",
    "\n",
    "#     all_phenotypes_keys = list(all_phenotypes.keys())  # Getting all the keys from your dictionary\n",
    "\n",
    "    \n",
    "#     for i in range (21): # each pheno appears 21 times in 210 pairs\n",
    "#         set_random_seed(i)\n",
    "#         for pheno in all_phenotypes_keys:\n",
    "\n",
    "#             k = pheno + ' - ' + str(i)\n",
    "#             pheno_data = np.array(all_phenotypes[pheno])\n",
    "\n",
    "#             print(k)\n",
    "\n",
    "#             simulationParams = {\n",
    "#                 'trials': 100,\n",
    "#                 'stimuli': {\n",
    "#                     'minContrast': raw_contrast_min,\n",
    "#                     'maxContrast': raw_contrast_max,\n",
    "#                     'contrastResolution': xs,\n",
    "#                     'minFrequency': raw_freq_min,\n",
    "#                     'maxFrequency': raw_freq_max,\n",
    "#                     'frequencyResolution': ys,\n",
    "#                 },\n",
    "#                 'parameters': None,\n",
    "#                 'd': 1,  # guess rate used by qcsf model           \n",
    "#                 'psiGamma': 0,\n",
    "#                 'psiLambda': 0,\n",
    "#                 'psiSigma': psi_sigma,\n",
    "#                 \"sigmoidType\": sigmoid_type,\n",
    "#                 'showPlots': False  # prevents plt from plotting every run \n",
    "#             }\n",
    "\n",
    "#             rmses, times, params = simulate.runSimulation(\n",
    "#                 trueThresholdCurve=pheno_data,\n",
    "#                 **simulationParams\n",
    "#             )        \n",
    "\n",
    "#             qcsf_results[k] = dict()\n",
    "#             qcsf_results[k]['params'] = params\n",
    "#             qcsf_results[k]['rmses'] = rmses\n",
    "\n",
    "\n",
    "#             # save to disk\n",
    "#             ensure_directory_exists(save_dir)\n",
    "#             with open(save_dir + \"qcsf_results.json\", 'w') as file:\n",
    "#                 json.dump(qcsf_results, file, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1335a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "363e6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    load_dir = save_dir\n",
    "else:\n",
    "    load_dir = 'C:/Repos/delete_me/Tables/data/'\n",
    "#     load_dir = './analysis/Figure06/2024-04-29_23-22-15/'\n",
    "\n",
    "if qcsf_train_mode:\n",
    "    qcsf_load_dir = save_dir\n",
    "else:\n",
    "    qcsf_load_dir = 'C:/Repos/delete_me/Tables/QCSF/'\n",
    "\n",
    "disjoint_runs = []\n",
    "conjoint_runs = []\n",
    "qcsf_runs = []\n",
    "for run in range(2):\n",
    "    \n",
    "    with open(f'{load_dir}Run_{run}/disjoint_results.pkl', 'rb') as file:\n",
    "        disjoint_runs.append(pickle.load(file))\n",
    "\n",
    "    with open(f'{load_dir}Run_{run}/conjoint_results.pkl', 'rb') as file:\n",
    "        conjoint_runs.append(pickle.load(file))\n",
    "        \n",
    "    with open(f'{qcsf_load_dir}run_{run}/qcsf_results.pkl', 'rb') as file:\n",
    "        qcsf_runs.append(pickle.load(file))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # load qcsf\n",
    "\n",
    "# if qcsf_train_mode:\n",
    "#     qcsf_load_dir = save_dir + \"qcsf_results.json\"\n",
    "# else:\n",
    "#     qcsf_load_dir = 'C:/Repos/delete_me/Figure06/2024-04-15_04-23-55/qcsf_results.json'\n",
    "\n",
    "# with open(qcsf_load_dir, 'r') as file:\n",
    "#     qcsf_results = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aa686df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Sample data\n",
    "# var = qcsf_runs[0][0][0]['GP_level_curve_y_values_on_grid_cols']\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Sample data\n",
    "# var = qcsf_runs[0][0][0]['GP_level_curve_y_values_on_grid_cols']\n",
    "# # Plot each list in var on a separate plot\n",
    "# for i, sublist in enumerate(var):\n",
    "#     plt.figure()  # Create a new figure for each plot\n",
    "#     plt.plot(sublist)\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.title(f'Plot of List {i+1}')\n",
    "\n",
    "# # Show all plots\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07dbe0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qcsf:\n",
      "qcsf_runs[run_idx][task_idx][pair_key][GP_level_curve_y_values_on_grid_cols][# sampled points (0-100)][grid_col_idx] = y-value\n",
      "qcsf_runs[run_idx][task_idx][pair_key][ground_truth_y_values_on_grid_cols][grid_col_idx] = y-value\n",
      "\n",
      "Type(qcsf_runs)=<class 'list'>, Num runs: len(disjoint_runs)=2\n",
      "a = qcsf_runs[0]\n",
      "Type(a)=<class 'list'>, Num tasks: len(a)=2\n",
      "Type(a[0])=<class 'dict'>, Num pairs: len(list(a[0].keys()))=210\n",
      "\n",
      "a[0].keys()=dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209])\n",
      "\n",
      "a[0][0].keys()=dict_keys(['random_seed', 'rmse_list', 'GP_level_curve_y_values_on_grid_cols', 'ground_truth_y_values_on_grid_cols'])\n",
      "\n",
      "Num samples points for this task/pair: 101\n"
     ]
    }
   ],
   "source": [
    "print('qcsf:')\n",
    "print('qcsf_runs[run_idx][task_idx][pair_key][GP_level_curve_y_values_on_grid_cols][# sampled points (0-100)][grid_col_idx] = y-value')\n",
    "print('qcsf_runs[run_idx][task_idx][pair_key][ground_truth_y_values_on_grid_cols][grid_col_idx] = y-value')\n",
    "print()\n",
    "print(f'Type(qcsf_runs)={type(qcsf_runs)}, Num runs: len(disjoint_runs)={len(qcsf_runs)}')\n",
    "a = qcsf_runs[0]\n",
    "print('a = qcsf_runs[0]')\n",
    "print(f'Type(a)={type(a)}, Num tasks: len(a)={len(a)}')\n",
    "print(f'Type(a[0])={type(a[0])}, Num pairs: len(list(a[0].keys()))={len(list(a[0].keys()))}')\n",
    "print()\n",
    "print(f'a[0].keys()={a[0].keys()}')\n",
    "print()\n",
    "print(f'a[0][0].keys()={a[0][0].keys()}')\n",
    "print()\n",
    "print('Num samples points for this task/pair:', len(a[0][0]['GP_level_curve_y_values_on_grid_cols']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c8888ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disjoint:\n",
      "disjoint_runs[run_idx][task_idx][pair_key][GP_level_curve_y_values_on_grid_cols][# sampled points (0-100)][grid_col_idx] = y-value\n",
      "disjoint_runs[run_idx][task_idx][pair_key][ground_truth_y_values_on_grid_cols][grid_col_idx] = y-value\n",
      "\n",
      "Type(disjoint_runs)=<class 'list'>, Num runs: len(disjoint_runs)=2\n",
      "a = disjoint_runs[0]\n",
      "Type(a)=<class 'list'>, Num tasks: len(a)=2\n",
      "Type(a[0])=<class 'dict'>, Num pairs: len(list(a[0].keys()))=210\n",
      "\n",
      "a[0].keys()=dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209])\n",
      "\n",
      "a[0][0].keys()=dict_keys(['training_seed', 'random_seed', 'X', 'y', 'zz', 'rmse_list', 'GP_level_curve_y_values_on_grid_cols', 'ground_truth_y_values_on_grid_cols'])\n",
      "\n",
      "Num samples points for this task/pair: 101\n"
     ]
    }
   ],
   "source": [
    "print('Disjoint:')\n",
    "print('disjoint_runs[run_idx][task_idx][pair_key][GP_level_curve_y_values_on_grid_cols][# sampled points (0-100)][grid_col_idx] = y-value')\n",
    "print('disjoint_runs[run_idx][task_idx][pair_key][ground_truth_y_values_on_grid_cols][grid_col_idx] = y-value')\n",
    "print()\n",
    "print(f'Type(disjoint_runs)={type(disjoint_runs)}, Num runs: len(disjoint_runs)={len(disjoint_runs)}')\n",
    "a = disjoint_runs[0]\n",
    "print('a = disjoint_runs[0]')\n",
    "print(f'Type(a)={type(a)}, Num tasks: len(a)={len(a)}')\n",
    "print(f'Type(a[0])={type(a[0])}, Num pairs: len(list(a[0].keys()))={len(list(a[0].keys()))}')\n",
    "print()\n",
    "print(f'a[0].keys()={a[0].keys()}')\n",
    "print()\n",
    "print(f'a[0][0].keys()={a[0][0].keys()}')\n",
    "print()\n",
    "print('Num samples points for this task/pair:', len(a[0][0]['GP_level_curve_y_values_on_grid_cols']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "734800e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjoint:\n",
      "conjoint_runs[run_idx][pair_key][GP_level_curve_y_values_on_grid_cols][task_idx][# sampled points (0-100)][grid_col_idx] = y-value\n",
      "conjoint_runs[run_idx][pair_key][ground_truth_y_values_on_grid_cols][task_idx][grid_col_idx] = y-value\n",
      "\n",
      "Type(conjoint_runs)=<class 'list'>, Num runs: len(conjoint_runs)=2\n",
      "a = conjoint_runs[0]\n",
      "Type(a[0])=<class 'dict'>, Num pairs: len(list(a.keys()))=210\n",
      "\n",
      "a[0].keys()=dict_keys(['training_seed', 'random_seeds', 'X', 'y', 'zz', 'task_indices', 'entropy_list', 'rmse_list', 'GP_level_curve_y_values_on_grid_cols', 'ground_truth_y_values_on_grid_cols'])\n",
      "\n",
      "Num samples points for this task/pair: 201\n",
      "Num cols: 91\n"
     ]
    }
   ],
   "source": [
    "print('Conjoint:')\n",
    "print('conjoint_runs[run_idx][pair_key][GP_level_curve_y_values_on_grid_cols][task_idx][# sampled points (0-100)][grid_col_idx] = y-value')\n",
    "print('conjoint_runs[run_idx][pair_key][ground_truth_y_values_on_grid_cols][task_idx][grid_col_idx] = y-value')\n",
    "print()\n",
    "print(f'Type(conjoint_runs)={type(conjoint_runs)}, Num runs: len(conjoint_runs)={len(conjoint_runs)}')\n",
    "a = conjoint_runs[0]\n",
    "print('a = conjoint_runs[0]')\n",
    "print(f'Type(a[0])={type(a[0])}, Num pairs: len(list(a.keys()))={len(list(a.keys()))}')\n",
    "print()\n",
    "print(f'a[0].keys()={a[0].keys()}')\n",
    "print()\n",
    "print('Num samples points for this task/pair:', len(a[0]['GP_level_curve_y_values_on_grid_cols'][0]))\n",
    "print('Num cols:', len(a[0]['GP_level_curve_y_values_on_grid_cols'][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1437b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys_to_remove = [key for key in qcsf_results.keys() if key.startswith('Quantile 1') or key.startswith('Quantile 5') or key.startswith('Quantile 3')]\n",
    "\n",
    "# for key in keys_to_remove:\n",
    "#     del qcsf_results[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2704ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in qcsf_results.keys():\n",
    "#     print(key, qcsf_results[key]['rmses'][99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9453c84",
   "metadata": {},
   "source": [
    "## Create RMSE plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a496b09",
   "metadata": {},
   "source": [
    "### Formatting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a6571f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure_width = 6.5  # inches\n",
    "# figure_height = figure_width / 2  # inches\n",
    "\n",
    "# dpi_val = 600              # graphics resolution\n",
    "# plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# title=\"RMSE Comparison\"\n",
    "# rmse_x_label = \"Sample Count\"\n",
    "# rmse_y_label = \"RMSE\"\n",
    "\n",
    "# legend_font_size = 8\n",
    "# tick_font_size = 8\n",
    "# label_font_size = 10\n",
    "# title_font_size = 12\n",
    "\n",
    "# # x_tick_labels = THESE ARE DEFINED AT THE BEGINNING\n",
    "# # y_tick_labels = THESE ARE DEFINED AT THE BEGINNING\n",
    "\n",
    "# DISJOINT, CONJOINT = \"disjoint\", \"conjoint\"\n",
    "# std_transparency = 0.2\n",
    "\n",
    "# rmse_x_ticks = [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200]\n",
    "# rmse_y_ticks = np.arange(0, .31, step=0.05)\n",
    "# rmse_x_ticks_min, rmse_x_ticks_max = rmse_x_ticks[0], rmse_x_ticks[-1]\n",
    "# rmse_y_ticks_min, rmse_y_ticks_max = rmse_y_ticks[0], rmse_y_ticks[-1]\n",
    "\n",
    "# axis_tick_params = {'axis':'both', 'which':'major', 'direction':'out', 'length': 2}\n",
    "\n",
    "# filename = 'Tables'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba623649",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19930a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for run in range(2):\n",
    "    \n",
    "#     print('################')\n",
    "#     print(f'Run {run}')\n",
    "#     print('###############')\n",
    "    \n",
    "#     conjoint_results_dicts = conjoint_runs[run]\n",
    "#     disjoint_results_dicts = disjoint_runs[run]\n",
    "\n",
    "#     num_pairs = len(conjoint_results_dicts.keys())\n",
    "\n",
    "#     sample_count = np.arange(1, num_conjoint_pts + 1)\n",
    "\n",
    "#     # Initialize arrays to store overall means and standard deviations\n",
    "#     overall_mean = {DISJOINT: np.zeros(num_conjoint_pts), CONJOINT: np.zeros(num_conjoint_pts)}\n",
    "#     overall_std = {DISJOINT: np.zeros(num_conjoint_pts), CONJOINT: np.zeros(num_conjoint_pts)}\n",
    "\n",
    "#     # Calculating the overall mean and standard deviation for each condition\n",
    "#     for condition, results_dicts, _ in [(DISJOINT, disjoint_results_dicts, 'blue'), (CONJOINT, conjoint_results_dicts, 'red')]:\n",
    "#         all_rmses = []\n",
    "#         for pair_idx in range(num_pairs):\n",
    "#             for task_idx in range(num_tasks):\n",
    "#                 if condition == DISJOINT:\n",
    "#                     rmse_list = np.repeat(results_dicts[task_idx][pair_idx]['rmse_list'], num_tasks)\n",
    "#                 elif condition == CONJOINT:\n",
    "#                     rmse_list = results_dicts[pair_idx]['rmse_list'][task_idx]\n",
    "#                 all_rmses.append(rmse_list)\n",
    "#         overall_mean[condition] = np.mean(all_rmses, axis=0)\n",
    "#         overall_std[condition] = np.std(all_rmses, axis=0)\n",
    "\n",
    "#     # Calculate the average RMSE values for each key in qcsf_results\n",
    "#     qcsf_rmses = []\n",
    "#     for key in qcsf_results.keys():\n",
    "#         rmses = qcsf_results[key]['rmses']\n",
    "#         qcsf_rmses.append(rmses)\n",
    "#     qcsf_mean = np.mean(qcsf_rmses, axis=0)\n",
    "#     qcsf_mean = np.repeat(qcsf_mean[:100], 2) # create the staircase\n",
    "#     qcsf_std = np.std(qcsf_rmses, axis=0)    \n",
    "\n",
    "#     # Set up the plot\n",
    "#     fig, ax = plt.subplots(figsize=(figure_width, figure_height))\n",
    "\n",
    "#     # Plotting\n",
    "#     for condition, color in [(DISJOINT, 'blue'), (CONJOINT, 'red'), (\"QCSF\", 'purple')]:\n",
    "#         if condition == \"QCSF\":\n",
    "#             mean = qcsf_mean\n",
    "#             std = qcsf_std\n",
    "#         else:\n",
    "#             mean = overall_mean[condition]\n",
    "#             std = overall_std[condition]\n",
    "#         ax.plot(sample_count, mean, label=condition, color=color)\n",
    "#         ax.fill_between(sample_count, mean + std, mean - std, alpha=std_transparency, color=color)\n",
    "\n",
    "#     # Setting plot attributes\n",
    "#     ax.tick_params(**axis_tick_params, labelsize=tick_font_size)\n",
    "#     ax.set_xlim(rmse_x_ticks_min, rmse_x_ticks_max)\n",
    "#     ax.set_ylim(rmse_y_ticks_min, rmse_y_ticks_max)\n",
    "#     ax.set_title(title, fontsize=title_font_size)\n",
    "#     ax.set_xlabel(rmse_x_label)\n",
    "#     ax.set_ylabel(rmse_y_label, fontsize=label_font_size)\n",
    "#     ax.legend(loc='upper right', fontsize=legend_font_size, frameon=False)\n",
    "#     ax.tick_params(axis='both', labelsize=tick_font_size)\n",
    "#     plt.setp(ax, xticks=rmse_x_ticks, yticks=rmse_y_ticks)\n",
    "\n",
    "#     plt.gcf().subplots_adjust(bottom=0.15)\n",
    "\n",
    "#     # Saving or showing the plot\n",
    "#     if save_plots_mode: \n",
    "#         full_save_dir = f'{save_dir}run_{run}'\n",
    "#         ensure_directory_exists(full_save_dir)\n",
    "#         plt.savefig(f\"{full_save_dir}/rmse_comparison.png\", dpi=dpi_val)\n",
    "#         plt.savefig(f\"{full_save_dir}/rmse_comparison.pdf\", dpi=dpi_val)\n",
    "#     if scrn_mode: \n",
    "#         plt.show()\n",
    "#     plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f61fff",
   "metadata": {},
   "source": [
    "## Make tables 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eab3953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "GP_key = 'GP_level_curve_y_values_on_grid_cols'\n",
    "ground_truth_key = 'ground_truth_y_values_on_grid_cols'\n",
    "\n",
    "num_runs = len(conjoint_runs) # 2\n",
    "num_pairs = len(conjoint_runs[0]) # 210\n",
    "num_tasks = len(conjoint_runs[0][0][GP_key]) # 2\n",
    "num_samples = len(conjoint_runs[0][0][GP_key][0]) # 201\n",
    "num_grid_cols = len(conjoint_runs[0][0][GP_key][0][0]) # 91\n",
    "\n",
    "'''\n",
    "Format of data\n",
    "\n",
    "y-value = conjoint_runs[run_idx][pair_idx]['GP_level_curve_y_values_on_grid_cols'][task_idx][# sampled points (0-200)][grid_col_idx (0-90)]\n",
    "y-value = conjoint_runs[run_idx][pair_idx]['ground_truth_y_values_on_grid_cols'][task_idx][grid_col_idx (0-90)]\n",
    "\n",
    "y-value = disjoint_runs[run_idx][task_idx][pair_idx]['GP_level_curve_y_values_on_grid_cols'][# sampled points (0-200)][grid_col_idx (0-90)]\n",
    "y-value = disjoint_runs[run_idx][task_idx][pair_idx]['ground_truth_y_values_on_grid_cols'][grid_col_idx (0-90)]\n",
    "'''\n",
    "def get_disjoint_y(run, pair, task, grid_col, sample, curve):\n",
    "    if curve == GP_key:\n",
    "        return disjoint_runs[run][task][pair][GP_key][sample][grid_col]\n",
    "    elif curve == ground_truth_key:\n",
    "        return disjoint_runs[run][task][pair][ground_truth_key][grid_col]\n",
    "    else:\n",
    "        raise 'ERROR, WRONG KEY'\n",
    "        \n",
    "def get_conjoint_y(run, pair, task, grid_col, sample, curve):\n",
    "    if curve == GP_key:\n",
    "        return conjoint_runs[run][pair][GP_key][task][sample][grid_col]\n",
    "    elif curve == ground_truth_key:\n",
    "        return conjoint_runs[run][pair][ground_truth_key][task][grid_col]\n",
    "    else:\n",
    "        raise 'ERROR, WRONG KEY'\n",
    "        \n",
    "        \n",
    "# Repeat the disjoint and qcsf data\n",
    "num_disjoint_samples = len(disjoint_runs[0][0][0][GP_key])\n",
    "for pair_index in range(num_pairs):\n",
    "    for task_index in range(num_tasks):\n",
    "        for run_index in range(num_runs):\n",
    "            disjoint_runs[run_index][task_index][pair_index][GP_key] = [\n",
    "                np.copy(disjoint_runs[run_index][task_index][pair_index][GP_key][sample])\n",
    "                for sample in range(num_disjoint_samples)\n",
    "                for i in range(2)\n",
    "            ][1:]\n",
    "            \n",
    "            qcsf_runs[run_index][task_index][pair_index][GP_key] = [\n",
    "                np.copy(qcsf_runs[run_index][task_index][pair_index][GP_key][sample])\n",
    "                for sample in range(num_disjoint_samples)\n",
    "                for i in range(2)\n",
    "            ][1:]\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "# put stuff in numpy so it's easier to work with\n",
    "np_disjoint_GP = np.empty((num_runs, num_pairs, num_tasks, num_samples, num_grid_cols))\n",
    "np_conjoint_GP = np.empty((num_runs, num_pairs, num_tasks, num_samples, num_grid_cols))\n",
    "np_ground_truth = np.empty((num_runs, num_pairs, num_tasks, num_samples, num_grid_cols))\n",
    "np_qcsf = np.empty((num_runs, num_pairs, num_tasks, num_samples, num_grid_cols))\n",
    "for run_index in range(num_runs):\n",
    "    for pair_index in range(num_pairs):\n",
    "        print(pair_index)\n",
    "        for task_index in range(num_tasks):\n",
    "            for sample_index in range(num_samples):\n",
    "                for grid_col_index in range(num_grid_cols):\n",
    "                \n",
    "                    np_disjoint_GP[run_index, pair_index, task_index, sample_index, grid_col_index] \\\n",
    "                    = disjoint_runs[run_index][task_index][pair_index][GP_key][sample_index][grid_col_index]\n",
    "                    \n",
    "                    np_conjoint_GP[run_index, pair_index, task_index, sample_index, grid_col_index] \\\n",
    "                    = conjoint_runs[run_index][pair_index][GP_key][task_index][sample_index][grid_col_index]  \n",
    "                \n",
    "                    np_ground_truth[run_index, pair_index, task_index, sample_index, grid_col_index] \\\n",
    "                    = conjoint_runs[run_index][pair_index][ground_truth_key][task_index][grid_col_index]   \n",
    "                    \n",
    "                    np_qcsf[run_index, pair_index, task_index, sample_index, grid_col_index] \\\n",
    "                    = qcsf_runs[run_index][task_index][pair_index][GP_key][sample_index][grid_col_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "862860b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axes: (run, pair, task, # sampled points, grid col)\n",
      "np_disjoint_GP dimensions: (2, 210, 2, 201, 91)\n",
      "np_conjoint_GP dimensions: (2, 210, 2, 201, 91)\n",
      "np_ground_truth dimensions: (2, 210, 2, 201, 91)\n",
      "np_qcsf dimensions: (2, 210, 2, 201, 91)\n"
     ]
    }
   ],
   "source": [
    "print('axes: (run, pair, task, # sampled points, grid col)')\n",
    "print(\"np_disjoint_GP dimensions:\", np_disjoint_GP.shape)\n",
    "print(\"np_conjoint_GP dimensions:\", np_conjoint_GP.shape)\n",
    "print(\"np_ground_truth dimensions:\", np_ground_truth.shape)\n",
    "print(\"np_qcsf dimensions:\", np_qcsf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d77a44a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic: Signed Difference (2nd-1st)\n",
      "  Curves: qCSF(run 0) vs qCSF(run 1)\n",
      "    Samples: 20 | Mean: 0.0035 | Std: 0.3013 | \n",
      "    Samples: 50 | Mean: 0.0029 | Std: 0.1972 | \n",
      "    Samples: 100 | Mean: 0.0013 | Std: 0.1623 | \n",
      "\n",
      "  Curves: Disjoint(run 0) vs Disjoint(run 1)\n",
      "    Samples: 20 | Mean: 0.0222 | Std: 0.4587 | \n",
      "    Samples: 50 | Mean: 0.0048 | Std: 0.2500 | \n",
      "    Samples: 100 | Mean: -0.0034 | Std: 0.1174 | \n",
      "\n",
      "  Curves: Conjoint(run 0) vs Conjoint(run 1)\n",
      "    Samples: 20 | Mean: -0.0108 | Std: 0.3494 | \n",
      "    Samples: 50 | Mean: -0.0082 | Std: 0.1862 | \n",
      "    Samples: 100 | Mean: 0.0011 | Std: 0.1086 | \n",
      "\n",
      "  Curves: qCSF(run 0) vs Ground Truth(run 0)\n",
      "    Samples: 20 | Mean: -0.0494 | Std: 0.2210 | \n",
      "    Samples: 50 | Mean: 0.0026 | Std: 0.1512 | \n",
      "    Samples: 100 | Mean: 0.0099 | Std: 0.1287 | \n",
      "\n",
      "  Curves: Disjoint(run 0) vs Ground Truth(run 0)\n",
      "    Samples: 20 | Mean: 0.0273 | Std: 0.3486 | \n",
      "    Samples: 50 | Mean: 0.0130 | Std: 0.1915 | \n",
      "    Samples: 100 | Mean: -0.0029 | Std: 0.0875 | \n",
      "\n",
      "  Curves: Conjoint(run 0) vs Ground Truth(run 0)\n",
      "    Samples: 20 | Mean: -0.0027 | Std: 0.2672 | \n",
      "    Samples: 50 | Mean: 0.0001 | Std: 0.1419 | \n",
      "    Samples: 100 | Mean: -0.0001 | Std: 0.0806 | \n",
      "\n",
      "Statistic: Absolute Difference\n",
      "  Curves: qCSF(run 0) vs qCSF(run 1)\n",
      "    Samples: 20 | Mean: 0.2218 | Std: 0.2041 | \n",
      "    Samples: 50 | Mean: 0.1317 | Std: 0.1468 | \n",
      "    Samples: 100 | Mean: 0.0970 | Std: 0.1302 | \n",
      "\n",
      "  Curves: Disjoint(run 0) vs Disjoint(run 1)\n",
      "    Samples: 20 | Mean: 0.3376 | Std: 0.3113 | \n",
      "    Samples: 50 | Mean: 0.1677 | Std: 0.1855 | \n",
      "    Samples: 100 | Mean: 0.0898 | Std: 0.0757 | \n",
      "\n",
      "  Curves: Conjoint(run 0) vs Conjoint(run 1)\n",
      "    Samples: 20 | Mean: 0.2510 | Std: 0.2433 | \n",
      "    Samples: 50 | Mean: 0.1292 | Std: 0.1343 | \n",
      "    Samples: 100 | Mean: 0.0828 | Std: 0.0703 | \n",
      "\n",
      "  Curves: qCSF(run 0) vs Ground Truth(run 0)\n",
      "    Samples: 20 | Mean: 0.1659 | Std: 0.1542 | \n",
      "    Samples: 50 | Mean: 0.1038 | Std: 0.1100 | \n",
      "    Samples: 100 | Mean: 0.0829 | Std: 0.0990 | \n",
      "\n",
      "  Curves: Disjoint(run 0) vs Ground Truth(run 0)\n",
      "    Samples: 20 | Mean: 0.2460 | Std: 0.2486 | \n",
      "    Samples: 50 | Mean: 0.1195 | Std: 0.1503 | \n",
      "    Samples: 100 | Mean: 0.0663 | Std: 0.0572 | \n",
      "\n",
      "  Curves: Conjoint(run 0) vs Ground Truth(run 0)\n",
      "    Samples: 20 | Mean: 0.1866 | Std: 0.1913 | \n",
      "    Samples: 50 | Mean: 0.0961 | Std: 0.1044 | \n",
      "    Samples: 100 | Mean: 0.0623 | Std: 0.0511 | \n",
      "\n",
      "Statistic: Root Mean Square Difference\n",
      "  Curves: qCSF(run 0) vs qCSF(run 1)\n",
      "    Samples: 20 | Value: 0.3014\n",
      "    Samples: 50 | Value: 0.1972\n",
      "    Samples: 100 | Value: 0.1624\n",
      "\n",
      "  Curves: Disjoint(run 0) vs Disjoint(run 1)\n",
      "    Samples: 20 | Value: 0.4592\n",
      "    Samples: 50 | Value: 0.2501\n",
      "    Samples: 100 | Value: 0.1174\n",
      "\n",
      "  Curves: Conjoint(run 0) vs Conjoint(run 1)\n",
      "    Samples: 20 | Value: 0.3495\n",
      "    Samples: 50 | Value: 0.1864\n",
      "    Samples: 100 | Value: 0.1086\n",
      "\n",
      "  Curves: qCSF(run 0) vs Ground Truth(run 0)\n",
      "    Samples: 20 | Value: 0.2265\n",
      "    Samples: 50 | Value: 0.1512\n",
      "    Samples: 100 | Value: 0.1291\n",
      "\n",
      "  Curves: Disjoint(run 0) vs Ground Truth(run 0)\n",
      "    Samples: 20 | Value: 0.3497\n",
      "    Samples: 50 | Value: 0.1920\n",
      "    Samples: 100 | Value: 0.0876\n",
      "\n",
      "  Curves: Conjoint(run 0) vs Ground Truth(run 0)\n",
      "    Samples: 20 | Value: 0.2672\n",
      "    Samples: 50 | Value: 0.1419\n",
      "    Samples: 100 | Value: 0.0806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DISJOINT = 'Disjoint'\n",
    "CONJOINT = 'Conjoint'\n",
    "GT = 'Ground Truth'\n",
    "QCSF = 'qCSF'\n",
    "\n",
    "stuff_to_run = [\n",
    "    ((QCSF, 0), (QCSF, 1)),\n",
    "    ((DISJOINT, 0), (DISJOINT, 1)),\n",
    "    ((CONJOINT, 0), (CONJOINT, 1)),\n",
    "    ((QCSF, 0), (GT, 0)),\n",
    "    ((DISJOINT, 0), (GT, 0)),\n",
    "    ((CONJOINT, 0), (GT, 0)),\n",
    "]\n",
    "\n",
    "samples_list = [20, 50, 100]\n",
    "\n",
    "# Statistics to calculate\n",
    "def signed_diff(a, b):\n",
    "    sign_diff = b - a\n",
    "    return {\n",
    "        'Mean': np.mean(sign_diff),\n",
    "        'Std': np.std(sign_diff)\n",
    "    }\n",
    "\n",
    "def absolute_diff(a, b):\n",
    "    abs_diff = np.abs(b - a)\n",
    "    return {\n",
    "        'Mean': np.mean(abs_diff),\n",
    "        'Std': np.std(abs_diff)\n",
    "    }\n",
    "    \n",
    "def root_mean_square_diff(a, b):\n",
    "    return np.sqrt(np.mean(np.square(b - a)))\n",
    "    \n",
    "statistics = {\n",
    "    'Signed Difference (2nd-1st)': signed_diff,\n",
    "    'Absolute Difference': absolute_diff,\n",
    "    'Root Mean Square Difference': root_mean_square_diff\n",
    "}\n",
    "\n",
    "condition_mapping = {\n",
    "    DISJOINT: np_disjoint_GP,\n",
    "    CONJOINT: np_conjoint_GP,\n",
    "    GT: np_ground_truth,\n",
    "    QCSF: np_qcsf\n",
    "}\n",
    "\n",
    "for stat_name, stat_func in statistics.items():\n",
    "    print(f'Statistic: {stat_name}')\n",
    "    \n",
    "    for stuff in stuff_to_run:\n",
    "        a_condition = stuff[0][0]\n",
    "        a_run = stuff[0][1]\n",
    "        b_condition = stuff[1][0]\n",
    "        b_run = stuff[1][1]\n",
    "        \n",
    "        print(f'  Curves: {a_condition}(run {a_run}) vs {b_condition}(run {b_run})')\n",
    "        \n",
    "        for samples in samples_list:\n",
    "            print(f'    Samples: {samples} | ', end='')\n",
    "            \n",
    "            a = condition_mapping[a_condition][a_run, :, :, samples, :]\n",
    "            b = condition_mapping[b_condition][b_run, :, :, samples, :]\n",
    "            just_GT = condition_mapping[GT][0, :, :, samples, :]\n",
    "\n",
    "            # Create a mask for NaN values\n",
    "            mask_a = ~np.isnan(a)\n",
    "            mask_b = ~np.isnan(b)\n",
    "            mask_just_GT = ~np.isnan(just_GT)\n",
    "            mask_combined = mask_a & mask_b & mask_just_GT\n",
    "\n",
    "            # Apply the mask to both arrays\n",
    "            a_filtered = a[mask_combined]\n",
    "            b_filtered = b[mask_combined]\n",
    "\n",
    "            # Calculate the statistic using the corresponding function from the dictionary\n",
    "            stat_result = stat_func(a_filtered, b_filtered)\n",
    "\n",
    "            if isinstance(stat_result, dict):\n",
    "                for k, v in stat_result.items():\n",
    "                    print(f'{k}: {v:.4f} | ', end='')\n",
    "            else:\n",
    "                print(f'Value: {stat_result:.4f}', end='')\n",
    "                \n",
    "            print()\n",
    "                    \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e800fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming np_disjoint_GP, np_conjoint_GP, and np_ground_truth are defined\n",
    "\n",
    "# # Get the actual dimensions from the arrays\n",
    "# num_samples = np_disjoint_GP.shape[3]  # Assuming the sample axis is the 4th axis\n",
    "# num_grid_cols = np_disjoint_GP.shape[4]  # Assuming the grid column axis is the 5th axis\n",
    "\n",
    "# # Define the number of samples per figure\n",
    "# samples_per_figure = 10\n",
    "\n",
    "# # Calculate the number of figures needed\n",
    "# num_figures = (num_samples + samples_per_figure - 1) // samples_per_figure\n",
    "\n",
    "# # Plot samples in batches of samples_per_figure\n",
    "# for fig_index in range(num_figures):\n",
    "#     start_sample = fig_index * samples_per_figure\n",
    "#     end_sample = min((fig_index + 1) * samples_per_figure, num_samples)\n",
    "    \n",
    "#     # Create the subplots for the current batch of samples\n",
    "#     fig, axs = plt.subplots(end_sample - start_sample, figsize=(8, 6 * (end_sample - start_sample)))\n",
    "    \n",
    "#     # Iterate over sample indices and plot each one on a separate subplot\n",
    "#     for sample_index in range(start_sample, end_sample):\n",
    "#         ax_index = sample_index - start_sample\n",
    "        \n",
    "#         # Plot np_disjoint_GP for the current sample index\n",
    "#         axs[ax_index].plot(np.arange(1, num_grid_cols + 1), np_disjoint_GP[0, 0, 0, sample_index],\n",
    "#                             label=f'Disjoint GP - Sample {sample_index}')\n",
    "        \n",
    "#         # Plot np_conjoint_GP for the current sample index\n",
    "#         axs[ax_index].plot(np.arange(1, num_grid_cols + 1), np_conjoint_GP[0, 0, 0, sample_index],\n",
    "#                             label=f'Conjoint GP - Sample {sample_index}')\n",
    "        \n",
    "#         # Plot np_ground_truth for the current sample index\n",
    "#         axs[ax_index].plot(np.arange(1, num_grid_cols + 1), np_ground_truth[0, 0, 0, sample_index],\n",
    "#                             label=f'Ground Truth - Sample {sample_index}')\n",
    "        \n",
    "#         # Set y-axis limits from 0 to 3\n",
    "#         axs[ax_index].set_ylim(0, 3)\n",
    "        \n",
    "#         # Add labels, title, legend, and grid to each subplot\n",
    "#         axs[ax_index].set_xlabel('X values')\n",
    "#         axs[ax_index].set_ylabel('Y values')\n",
    "#         axs[ax_index].set_title(f'Sample {sample_index}')\n",
    "#         axs[ax_index].legend()\n",
    "#         axs[ax_index].grid(True)\n",
    "    \n",
    "#     # Adjust layout and show the plot\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
